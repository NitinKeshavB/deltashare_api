---
description: Azure deployment patterns - Web App, Queues, environment configuration, and CI/CD
globs: ["**/deploy/**/*", "**/azure/**/*", "**/.github/**/*", "**/settings.py"]
alwaysApply: false
---

# Azure Deployment Patterns

## Azure Service Architecture

```
┌─────────────────────────────────────────────────────────────────┐
│                        Azure Resource Group                      │
├─────────────────────────────────────────────────────────────────┤
│                                                                  │
│  ┌──────────────┐    ┌──────────────┐    ┌──────────────┐       │
│  │  Azure Web   │    │   Azure      │    │   Azure      │       │
│  │  App (API)   │───▶│  PostgreSQL  │    │   Queue      │       │
│  └──────────────┘    └──────────────┘    └──────────────┘       │
│         │                                       │                │
│         │            ┌──────────────┐           │                │
│         └───────────▶│  Azure App   │◀──────────┘                │
│                      │  Config      │                            │
│                      └──────────────┘                            │
│                                                                  │
│  ┌──────────────┐    ┌──────────────┐                           │
│  │  Azure Key   │    │  Databricks  │                           │
│  │  Vault       │    │  Workspace   │                           │
│  └──────────────┘    └──────────────┘                           │
│                                                                  │
└─────────────────────────────────────────────────────────────────┘
```

## Environment Configuration

### Environment Variables Structure

```python
# settings.py
from pydantic_settings import BaseSettings, SettingsConfigDict
from typing import Optional

class Settings(BaseSettings):
    """Application settings loaded from environment."""

    # Environment
    environment: str = "dev"  # dev, uat, prod
    debug: bool = False

    # Databricks
    dltshr_workspace_url: str
    client_id: str = ""
    client_secret: str = ""
    account_id: str = ""

    # Database (Azure PostgreSQL)
    db_host: str = "localhost"
    db_port: int = 5432
    db_name: str = "deltashare"
    db_user: str = ""
    db_password: str = ""
    db_ssl_mode: str = "require"

    # Azure Queue Storage
    azure_queue_connection_string: str = ""
    azure_queue_name: str = "deltashare-jobs"

    # Azure App Configuration
    azure_app_config_endpoint: str = ""

    model_config = SettingsConfigDict(
        case_sensitive=False,
        env_file=".env",
        env_file_encoding="utf-8",
        extra="ignore",
    )
```

### Environment Files

```bash
# .env.dev - Local development (not committed)
ENVIRONMENT=dev
DEBUG=true
DLTSHR_WORKSPACE_URL=https://adb-dev.azuredatabricks.net/
DB_HOST=localhost
DB_NAME=deltashare_dev

# .env.uat - UAT environment (not committed)
ENVIRONMENT=uat
DEBUG=false
DLTSHR_WORKSPACE_URL=https://adb-uat.azuredatabricks.net/
DB_HOST=deltashare-uat.postgres.database.azure.com

# .env.prod - Production (not committed)
ENVIRONMENT=prod
DEBUG=false
DLTSHR_WORKSPACE_URL=https://adb-prod.azuredatabricks.net/
DB_HOST=deltashare-prod.postgres.database.azure.com
```

### Environment Selection
```python
import os
from dotenv import load_dotenv

def load_environment():
    """Load environment-specific configuration."""
    env = os.getenv("ENVIRONMENT", "dev")
    env_file = f".env.{env}"

    if os.path.exists(env_file):
        load_dotenv(env_file)
    else:
        load_dotenv(".env")  # Fallback
```

## Azure Web App Configuration

### Application Settings (Azure Portal / ARM)

| Setting                          | Dev  | UAT   | Prod  |
| -------------------------------- | ---- | ----- | ----- |
| `ENVIRONMENT`                    | dev  | uat   | prod  |
| `DEBUG`                          | true | false | false |
| `WEBSITES_PORT`                  | 8000 | 8000  | 8000  |
| `SCM_DO_BUILD_DURING_DEPLOYMENT` | true | true  | true  |

### Startup Command
```bash
# For Azure Web App startup
gunicorn -w 4 -k uvicorn.workers.UvicornWorker src.dbrx_api.main:create_app --bind 0.0.0.0:8000
```

### requirements.txt for Deployment
```txt
# requirements.txt (generated from pyproject.toml)
fastapi>=0.100.0
uvicorn[standard]>=0.23.0
gunicorn>=21.0.0
pydantic>=2.0.0
pydantic-settings>=2.0.0
sqlalchemy[asyncio]>=2.0.0
asyncpg>=0.28.0
alembic>=1.12.0
databricks-sdk>=0.12.0
azure-storage-queue>=12.0.0
azure-identity>=1.14.0
python-dotenv>=1.0.0
```

## Azure Queue Integration

### Queue Client Setup
```python
# services/queue_client.py
from azure.storage.queue import QueueClient
from azure.identity import DefaultAzureCredential

from dbrx_api.settings import Settings

def get_queue_client(settings: Settings) -> QueueClient:
    """Create Azure Queue client."""
    if settings.azure_queue_connection_string:
        return QueueClient.from_connection_string(
            settings.azure_queue_connection_string,
            queue_name=settings.azure_queue_name,
        )
    else:
        # Use managed identity in Azure
        credential = DefaultAzureCredential()
        return QueueClient(
            account_url=f"https://{settings.azure_storage_account}.queue.core.windows.net",
            queue_name=settings.azure_queue_name,
            credential=credential,
        )
```

### Queue Message Patterns
```python
# services/job_queue.py
import json
from datetime import datetime
from typing import Any, Dict

from azure.storage.queue import QueueClient

class JobQueue:
    """Queue for async job processing."""

    def __init__(self, queue_client: QueueClient):
        self.client = queue_client

    async def enqueue_job(
        self,
        job_type: str,
        payload: Dict[str, Any],
        visibility_timeout: int = 0,
    ) -> str:
        """Add job to queue."""
        message = {
            "job_type": job_type,
            "payload": payload,
            "created_at": datetime.utcnow().isoformat(),
        }
        result = self.client.send_message(
            json.dumps(message),
            visibility_timeout=visibility_timeout,
        )
        return result.id

    async def dequeue_job(self, visibility_timeout: int = 30):
        """Get next job from queue."""
        messages = self.client.receive_messages(
            max_messages=1,
            visibility_timeout=visibility_timeout,
        )
        for msg in messages:
            return json.loads(msg.content), msg
        return None, None

    async def complete_job(self, message):
        """Remove completed job from queue."""
        self.client.delete_message(message)
```

### Job Types
```python
# Job type constants
JOB_TYPE_SYNC_TABLE = "sync_table"
JOB_TYPE_CREATE_SHARE = "create_share"
JOB_TYPE_NOTIFY_RECIPIENT = "notify_recipient"
JOB_TYPE_AUDIT_CLEANUP = "audit_cleanup"
```

## Service Principal Authentication

### Azure AD App Registration
```python
# auth/azure_auth.py
from azure.identity import (
    DefaultAzureCredential,
    ClientSecretCredential,
    ManagedIdentityCredential,
)

from dbrx_api.settings import Settings

def get_azure_credential(settings: Settings):
    """Get Azure credential based on environment."""
    if settings.environment == "dev":
        # Use client secret for local development
        return ClientSecretCredential(
            tenant_id=settings.azure_tenant_id,
            client_id=settings.azure_client_id,
            client_secret=settings.azure_client_secret,
        )
    else:
        # Use managed identity in Azure
        return ManagedIdentityCredential()

def get_default_credential():
    """Get DefaultAzureCredential (tries multiple auth methods)."""
    return DefaultAzureCredential()
```

## Deployment Scripts

### GitHub Actions Workflow
```yaml
# .github/workflows/deploy.yml
name: Deploy to Azure

on:
  push:
    branches: [main]
  workflow_dispatch:
    inputs:
      environment:
        description: 'Deployment environment'
        required: true
        type: choice
        options: [dev, uat, prod]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: '3.12'
      - run: make install
      - run: make lint-ci
      - run: make test-ci

  deploy:
    needs: test
    runs-on: ubuntu-latest
    environment: ${{ github.event.inputs.environment || 'dev' }}
    steps:
      - uses: actions/checkout@v4
      - uses: azure/login@v1
        with:
          creds: ${{ secrets.AZURE_CREDENTIALS }}
      - uses: azure/webapps-deploy@v2
        with:
          app-name: deltashare-api-${{ env.ENVIRONMENT }}
          package: .
```

### Database Migration in CI/CD
```yaml
# Add to deploy job
- name: Run Alembic migrations
  run: |
    pip install -e ".[dev]"
    alembic upgrade head
  env:
    DATABASE_URL: ${{ secrets.DATABASE_URL }}
```

## Health Checks

### Health Endpoint
```python
# routes_health.py
from fastapi import APIRouter, status
from fastapi.responses import JSONResponse

ROUTER_HEALTH = APIRouter(tags=["Health"])

@ROUTER_HEALTH.get("/health")
async def health_check():
    """Basic health check endpoint."""
    return {"status": "healthy"}

@ROUTER_HEALTH.get("/health/ready")
async def readiness_check(request: Request):
    """Readiness check - verify all dependencies."""
    settings = request.app.state.settings
    checks = {
        "database": await check_database_connection(settings),
        "databricks": await check_databricks_connection(settings),
    }
    all_healthy = all(checks.values())
    return JSONResponse(
        status_code=status.HTTP_200_OK if all_healthy else status.HTTP_503_SERVICE_UNAVAILABLE,
        content={"status": "ready" if all_healthy else "not ready", "checks": checks},
    )
```

## Logging Configuration

```python
# logging_config.py
import logging
import sys

def configure_logging(environment: str):
    """Configure logging for the application."""
    log_level = logging.DEBUG if environment == "dev" else logging.INFO

    logging.basicConfig(
        level=log_level,
        format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
        handlers=[
            logging.StreamHandler(sys.stdout),
        ],
    )

    # Reduce noise from third-party libraries
    logging.getLogger("azure").setLevel(logging.WARNING)
    logging.getLogger("urllib3").setLevel(logging.WARNING)
```

## Secrets Management with Azure Key Vault

### How Key Vault Integration Works

The application automatically loads secrets from Azure Key Vault when deployed:

1. **Local Development**: Uses `.env` file (no `AZURE_KEYVAULT_URL` set)
2. **Azure Deployment**: Set `AZURE_KEYVAULT_URL` app setting, all secrets loaded from Key Vault

```mermaid
flowchart TD
    Start[App Starts] --> Check{AZURE_KEYVAULT_URL set?}
    Check -->|Yes| LoadKV[Load ALL secrets from Key Vault]
    Check -->|No| LoadEnv[Load from .env file]
    LoadKV --> SetEnv[Set as environment variables]
    LoadEnv --> Settings[Initialize Settings class]
    SetEnv --> Settings
```

### Key Vault Client Module

Located at `src/dbrx_api/keyvault/client.py`:

```python
from dbrx_api.keyvault import load_secrets_from_keyvault

def create_app(settings: Settings | None = None) -> FastAPI:
    # Load secrets BEFORE Settings() initialization
    load_secrets_from_keyvault()

    settings = settings or Settings()
    # ... rest of app
```

### Secret Naming Convention

Key Vault secrets use hyphens, converted to UPPER_SNAKE_CASE for environment variables:

| Key Vault Secret Name          | Environment Variable           |
| ------------------------------ | ------------------------------ |
| `client-id`                    | `CLIENT_ID`                    |
| `client-secret`                | `CLIENT_SECRET`                |
| `account-id`                   | `ACCOUNT_ID`                   |
| `azure-storage-account-url`    | `AZURE_STORAGE_ACCOUNT_URL`    |
| `postgresql-connection-string` | `POSTGRESQL_CONNECTION_STRING` |
| `enable-blob-logging`          | `ENABLE_BLOB_LOGGING`          |
| `enable-postgresql-logging`    | `ENABLE_POSTGRESQL_LOGGING`    |
| `postgresql-min-log-level`     | `POSTGRESQL_MIN_LOG_LEVEL`     |
| `azure-storage-logs-container` | `AZURE_STORAGE_LOGS_CONTAINER` |
| `postgresql-log-table`         | `POSTGRESQL_LOG_TABLE`         |

**Note**: `dltshr-workspace-url` is NOT stored in Key Vault - it comes from the `X-Workspace-URL` request header per API call.

### Azure Web App Configuration for Key Vault

Only ONE App Setting required in Azure Portal:

```
AZURE_KEYVAULT_URL=https://deltashare-kv-prod.vault.azure.net/
```

All other secrets are loaded automatically from Key Vault.

### Azure Setup Requirements

1. **Enable Managed Identity** on Azure Web App:
   - Go to Web App → Identity → System assigned → Status: On

2. **Grant Key Vault Access**:
   - Go to Key Vault → Access policies → Add Access Policy
   - Secret permissions: Get, List
   - Select principal: Your Web App's Managed Identity

3. **Add Secrets to Key Vault**:
   - Use hyphenated names (e.g., `client-id`, `client-secret`)
   - All secrets from your `.env` file should be added

### Local Development with Key Vault (Optional)

If you want to test Key Vault locally:

```bash
# Option 1: Azure CLI authentication
az login
export AZURE_KEYVAULT_URL=https://deltashare-kv-dev.vault.azure.net/

# Option 2: Service Principal (in .env)
AZURE_KEYVAULT_URL=https://deltashare-kv-dev.vault.azure.net/
AZURE_CLIENT_ID=<sp-client-id>
AZURE_CLIENT_SECRET=<sp-secret>
AZURE_TENANT_ID=<tenant-id>
```

### Fetching Individual Secrets On-Demand

```python
from dbrx_api.keyvault.client import get_secret_from_keyvault

# Returns None if Key Vault not configured (local dev)
secret_value = get_secret_from_keyvault("my-secret-name")
```

### Required Secrets Summary

| Secret                         | Description                            | Required | Example Value                  |
| ------------------------------ | -------------------------------------- | -------- | ------------------------------ |
| `client-id`                    | Databricks service principal client ID | Yes      | `e04058ec-8264-...`            |
| `client-secret`                | Databricks service principal secret    | Yes      | `abc~xyz123...`                |
| `account-id`                   | Databricks account ID                  | Yes      | `12345678-1234-...`            |
| `azure-storage-account-url`    | Blob storage URL for logs              | No       | `https://storage.blob...`      |
| `postgresql-connection-string` | Database connection string             | No       | `postgresql://user:pass@...`   |
| `enable-blob-logging`          | Enable Azure Blob logging              | No       | `true` or `false`              |
| `enable-postgresql-logging`    | Enable PostgreSQL logging              | No       | `true` or `false`              |
| `postgresql-min-log-level`     | Minimum log level for PostgreSQL       | No       | `WARNING`, `ERROR`, `CRITICAL` |
| `azure-storage-logs-container` | Blob container name for logs           | No       | `deltashare-logs`              |
| `postgresql-log-table`         | PostgreSQL table name for logs         | No       | `application_logs`             |

**Excluded from Key Vault** (comes from request headers):
- `X-Workspace-URL` header → Databricks workspace URL per request
